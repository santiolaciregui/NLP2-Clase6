{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:red\">INSTANTIATION OF THE LLM MODEL AND THE EMBEDDING</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_community\n",
    "import os\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_TRACING_V2 = os.environ[\"LANGCHAIN_TRACING_V2\"]\n",
    "LANGCHAIN_API_KEY = os.environ[\"LANGCHAIN_API_KEY\"]\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model='gpt-3.5-turbo',\n",
    "    temperature = 0,\n",
    "    streaming = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EMBEDDINGS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "## Embedding Techinque of OPENAI\n",
    "embed_model=OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "print(len(embed_model.embed_query('hola')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## READ THE DIRECTORY AND LOAD THE FILE\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "# read documents\n",
    "def read_doc(directory):\n",
    "    file_loader=PyPDFDirectoryLoader(directory)\n",
    "    documents = file_loader.load()\n",
    "    return documents\n",
    "dir=r'C:\\Users\\Aulamultimedia\\Documents\\practicas\\eguins2\\pdf_tesis/'\n",
    "doc=read_doc(dir)\n",
    "\n",
    "dir_cv=r'C:\\Users\\Aulamultimedia\\Documents\\practicas\\eguins2\\pdf_cv/'\n",
    "doc_cv=read_doc(dir_cv)\n",
    "\n",
    "total=doc+doc_cv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_data(docs, chunk_size=800, chunk_overlap=50):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    doc=text_splitter.split_documents(docs)\n",
    "    return doc\n",
    "\n",
    "documents=chunk_data(docs=total,chunk_size=3000, chunk_overlap=50)\n",
    "# documents_cv=chunk_data(docs=doc_cv,chunk_size=3000, chunk_overlap=50)\n",
    "\n",
    "type(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:red\">LOAD THE DOCUMENTS AND VECTORS TO PINESTORE DB</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONNECT WITH PINECONE DATABASE\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "PINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "#Connect to DB Pinecone\n",
    "pc=Pinecone(api_key=PINECONE_API_KEY)\n",
    "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
    "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
    "spec = ServerlessSpec(cloud=cloud, region=region)\n",
    "index_name = 'eguins'\n",
    "\n",
    "if index_name in pc.list_indexes().names():\n",
    "  pc.delete_index(index_name)\n",
    "  print(\"index {} borrado\".format(index_name))\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    # if does not exist, create index\n",
    "    print(\"index creado con el nombre: {}\".format(index_name))\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=3072,  # dimensionality of text-embedding models/embedding-001\n",
    "        metric='cosine',\n",
    "        spec=spec\n",
    "        )\n",
    "else:\n",
    "    print(\"el index con el nombre {} ya estaba creado\".format(index_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UPSERT THE VECTORS IN TO THE PINECONE DATABASE\n",
    "\n",
    "import time\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "namespace = \"espacio\"\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=documents,\n",
    "    index_name=index_name,\n",
    "    embedding=embed_model, \n",
    "    namespace=namespace\n",
    ")\n",
    "print(\"upserted values to {} index\".format(index_name))\n",
    "\n",
    "time.sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:red\">RETRIEVE AND SEARCH INTO THE CREATED PINECONE DATABASES</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "PINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")\n",
    "pc=Pinecone(api_key=PINECONE_API_KEY)\n",
    "index_name = 'eguins'\n",
    "index_name_cv = 'eguinscv'\n",
    "namespace = \"espacio\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = PineconeVectorStore(\n",
    "    index_name=index_name,\n",
    "    embedding=embed_model,\n",
    "    namespace=namespace,\n",
    ")\n",
    "retriever=vectorstore.as_retriever()\n",
    "\n",
    "vectorstore_cv = PineconeVectorStore(\n",
    "    index_name=index_name_cv,\n",
    "    embedding=embed_model,\n",
    "    namespace=namespace,\n",
    ")\n",
    "retriever_cv=vectorstore_cv.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"in which companies did ezequiel used to work\"\n",
    "vectorstore.similarity_search(query, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA  \n",
    "\n",
    "query = \"does Ezequiel have any hands-on experience\"\n",
    "\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(  \n",
    "    llm=chat,  \n",
    "    chain_type=\"stuff\",  \n",
    "    retriever=vectorstore.as_retriever()  \n",
    ")  \n",
    "\n",
    "qa_cv = RetrievalQA.from_chain_type(  \n",
    "    llm=chat,  \n",
    "    chain_type=\"stuff\",  \n",
    "    retriever=vectorstore_cv.as_retriever()  \n",
    ") \n",
    "result = qa_cv.invoke(query)\n",
    "\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
